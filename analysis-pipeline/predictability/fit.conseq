# Three Steps:
# 1. Generate a daintree input config file for each model and screen
# 2. Run the model fitting
# 3. Combine the output config files

# todo: fix docker image such that default python is python3, and make sparkles in path
let sparkes_path = "/install/sparkles/bin/sparkles"
let daintree_docker_image = "us-central1-docker.pkg.dev/cds-docker-containers/docker/daintree:test"

rule process_model_config:
    inputs:
        # Model config yaml file
        model_config=fileref("model-config.yaml"),

        # Target matrices
        crispr_gene_effect={"type": "target_matrix", "label": "crispr_gene_effect"},
        rnai={"type": "target_matrix", "label": "rnai"},
        oncref={"type": "target_matrix", "label": "oncref"},

        # Features
        lineage={"type": "feature", "label": "lineage"},
        crispr_confounder={"type": "feature", "label": "crispr_confounder"},
        rnai_confounder={"type": "feature", "label": "rnai_confounder"},
        oncref_confounder={"type": "feature", "label": "oncref_confounder"},
        driver_events={"type": "feature", "label": "driver_events"},
        armlevel_cna={"type": "feature", "label": "armlevel_cna"},
        cytoband_cn={"type": "feature", "label": "cytoband_cn"},
        genetic_signature={"type": "feature", "label": "genetic_signature"},
        mutations_hotspot={"type": "feature", "label": "mutations_hotspot"},
        mutations_damaging={"type": "feature", "label": "mutations_damaging"},
        gene_cn={"type": "feature", "label": "gene_cn"},
        loh={"type": "feature", "label": "loh"},
        rnaseq={"type": "feature", "label": "rnaseq"},

        # Script to generate daintree input config file
        script=fileref("scripts/generate_daintree_input_configs.py"),

    run "python" with """
    import json

    config_dict = {{inputs}}
    with open("daintree_input_config.json", 'w') as f:
        json.dump(config_dict, f, indent=2)   

    """
    run "python3 {{ inputs.script.filename }} --model_config {{ inputs.model_config.filename }} --input_config 'daintree_input_config.json'"


rule run_fit_models:
    resources: {'slots': "0.5"} # let up to 2 of these run in parallel
    inputs:
        daintree_input_config={
          "type": "daintree_input_config"
          },
        release_taiga_id={
          "type": "release_taiga_id"
          },
        sparkles_config=fileref("sparkles-config"),
        create_workflow_script=fileref("scripts/create_daintree_workflow.py")
#    run "cp {{ inputs.daintree_input_config.filename }} {{ inputs.daintree_input_config.screen_name }}_{{ inputs.daintree_input_config.model_name }}.json"
    run "python3 {{ inputs.create_workflow_script.filename }} --config {{ inputs.daintree_input_config.filename }} --out workflow.json --nfolds 5 --models-per-task 5 --test-first-n-tasks 10"
    run "cp {{ inputs.sparkles_config.filename }} .sparkles"
    run "{{config.sparkles_path}} workflow run --retry --add-hash-to-job-id -i {{ config.daintree_docker_image }} daintree-fit workflow.json"
    # reformat daintree-output into artifacts conseq will understand
    run "python3" with """
    import json

    with open("daintree-output.json", "rt") as fd:
        result = json.load(fd)

    with open("outputs.json", "wt") as fd:
        fd.write(json.dumps(
            {"outputs": [
                {
                    "type": "daintree_output",
                    "model_name": {{ inputs.daintree_input_config.model_name | quoted }},
                    "screen_name": {{ inputs.daintree_input_config.screen_name | quoted }},
                    "ensemble_filename" : {"$file_url": result["ensemble_path"]},
                    "predictions_filename": {"$file_url": result["predictions_path"]}}
            ]}))
    """

# this rule won't actually run because I've changed the output type
rule combine_output_configs:
    inputs:
        daintree_output_config = all{
          "type": "daintree_output_config"
          }
    outputs:
        {
          "type": "combined_daintree_output_config",
          "filename": {"$filename": "combined_daintree_output_config.json"}
        }
    run "python" with """
        import json
        import os

        def merge_json_files(json_files):
            combined = {}
            
            for file_path in json_files:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    
                model_name = list(data.keys())[0]
                screen_name = data[model_name]["input"]["screen_name"]
                
                # Initialize the screen in combined if it doesn't exist
                if screen_name not in combined:
                    combined[screen_name] = {}
                    
                # Add the model data to the appropriate screen
                combined[screen_name][model_name] = data[model_name]
            
            return combined        

        artifacts = {{ inputs.daintree_output_config }}
        list_of_files = [artifact['filename'] for artifact in artifacts]

        combined_output_config = merge_json_files(list_of_files)

        try:
            with open("combined_daintree_output_config.json", 'w') as f:
                json.dump(combined_output_config, f, indent=2)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in combined output config: {e}")
            raise

        # Publish the combined output config file to gcp bucket
        import subprocess
        import datetime

        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/etc/google_default_creds.json"
        bucket_name = "preprocessing-pipeline-outputs"
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')

        subprocess.run([
            "gsutil", "-o", f"Credentials:gs_service_key_file=/etc/google_default_creds.json",
            "cp", "combined_daintree_output_config.json", 
            f"gs://{bucket_name}/analysis-pipeline/combined_daintree_output_config-{timestamp}.json"
        ])
        
    """

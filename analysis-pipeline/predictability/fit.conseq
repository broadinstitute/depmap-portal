# Three Steps:
# 1. Generate a daintree input config file for each model and screen
# 2. Run the model fitting
# 3. Combine the output config files

rule process_model_config:
    inputs:
        # Model config yaml file
        model_config=fileref("model-config.yaml"),

        # Target matrices
        crispr_gene_effect={"type": "target_matrix", "label": "crispr_gene_effect"},
        rnai={"type": "target_matrix", "label": "rnai"},
        oncref={"type": "target_matrix", "label": "oncref"},

        # Features
        lineage={"type": "feature", "label": "lineage"},
        crispr_confounder={"type": "feature", "label": "crispr_confounder"},
        rnai_confounder={"type": "feature", "label": "rnai_confounder"},
        oncref_confounder={"type": "feature", "label": "oncref_confounder"},
        driver_events={"type": "feature", "label": "driver_events"},
        armlevel_cna={"type": "feature", "label": "armlevel_cna"},
        cytoband_cn={"type": "feature", "label": "cytoband_cn"},
        genetic_signature={"type": "feature", "label": "genetic_signature"},
        mutations_hotspot={"type": "feature", "label": "mutations_hotspot"},
        mutations_damaging={"type": "feature", "label": "mutations_damaging"},
        gene_cn={"type": "feature", "label": "gene_cn"},
        loh={"type": "feature", "label": "loh"},
        rnaseq={"type": "feature", "label": "rnaseq"},

        # Script to generate daintree input config file
        script=fileref("scripts/generate_daintree_input_configs.py"),

    run "python" with """
    import json

    config_dict = {{inputs}}
    with open("daintree_input_config.json", 'w') as f:
        json.dump(config_dict, f, indent=2)   

    """
    run "python {{ inputs.script.filename }} --model_config {{ inputs.model_config.filename }} --input_config 'daintree_input_config.json'"


rule run_fit_models:
    resources: {'slots': "0.5"} # let up to 20 of these run in parallel
    inputs:
        daintree_input_config={
          "type": "daintree_input_config"
          },
        release_taiga_id={
          "type": "release_taiga_id"
          },
        sparkles_config=fileref("sparkles-config", copy_to="sparkles-config")
    outputs:
        {
          "type": "daintree_output_config",
          "name": "{{ inputs.daintree_input_config.label }}",
          "filename": {"$filename": "daintree_output_config.json"}
        }
    run "python" with """
      import subprocess
      import os
      import glob
      import json
      import shutil

      # Get the input config file and copy it to current directory
      input_config_filepath = "{{ inputs.daintree_input_config.filename }}"
      local_input_config = "{{ inputs.daintree_input_config.label }}.json"
      
      # Copy the input config file to the current directory
      shutil.copy(input_config_filepath, local_input_config)

      relative_path = os.path.relpath(os.getcwd(), '/work/analysis-pipeline')
      host_work_dir = os.environ.get('HOST_WORKSPACE_PATH', '/data1/jenkins/workspace/Analysis_Pipeline/analysis-pipeline')
      host_current_dir = os.path.join(host_work_dir, relative_path)
        

      docker_command = [
        "docker", "run",
        "--rm",
        "-v", f"{host_current_dir}:/daintree",
        "-v", f"/home/ubuntu/.taiga/token:/root/.taiga/token",
        "-v", f"/home/ubuntu/.sparkles-cache/service-keys/broad-achilles.json:/root/.sparkles-cache/service-keys/broad-achilles.json",
        "--entrypoint", "/bin/bash",
        "us.gcr.io/broad-achilles/daintree:v1",
        "-c", f"mkdir -p /daintree/daintree_scripts/ && cp -r /daintree_scripts/* /daintree/daintree_scripts/ && /install/depmap-py/bin/python3.9 -u /daintree/daintree_scripts/run_fit_models.py collect-and-fit \
        --input-config /daintree/{local_input_config} \
        --sparkles-config /daintree/sparkles-config \
        --out /daintree/output_data \
        --test \"True\" \
        --skipfit \"False\" \
        --upload-to-taiga \"{{ inputs.release_taiga_id.dataset_id }}\""
      ]

      subprocess.run(
          docker_command,
          check=True
      )
      
      # Find the output config file using glob
      output_config_files = glob.glob(os.path.join(os.getcwd(), "output_data", "output_config_files", "*.json"))
      if not output_config_files:
          raise FileNotFoundError("No output config files found")
      
      # Use the first, there should only be one matching file
      output_config_file = output_config_files[0]
      
      try:
          with open(output_config_file, 'r') as f:
              output_config = json.load(f)
          with open("daintree_output_config.json", 'w') as f:
              json.dump(output_config, f, indent=2)
      except json.JSONDecodeError as e:
          logger.error(f"Invalid JSON in output config: {e}")
          raise
    """


rule combine_output_configs:
    inputs:
        daintree_output_config = all{
          "type": "daintree_output_config"
          }
    outputs:
        {
          "type": "combined_daintree_output_config",
          "filename": {"$filename": "combined_daintree_output_config.json"}
        }
    run "python" with """
        import json
        import os

        def merge_json_files(json_files):
            combined = {}
            
            for file_path in json_files:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    
                model_name = list(data.keys())[0]
                screen_name = data[model_name]["input"]["screen_name"]
                
                # Initialize the screen in combined if it doesn't exist
                if screen_name not in combined:
                    combined[screen_name] = {}
                    
                # Add the model data to the appropriate screen
                combined[screen_name][model_name] = data[model_name]
            
            return combined        

        artifacts = {{ inputs.daintree_output_config }}
        list_of_files = [artifact['filename'] for artifact in artifacts]

        combined_output_config = merge_json_files(list_of_files)

        try:
            with open("combined_daintree_output_config.json", 'w') as f:
                json.dump(combined_output_config, f, indent=2)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in combined output config: {e}")
            raise

        # Publish the combined output config file to gcp bucket
        import subprocess
        import os
        import datetime

        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/etc/google_default_creds.json"
        bucket_name = "preprocessing-pipeline-outputs"
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')

        subprocess.run([
            "gsutil", "-o", f"Credentials:gs_service_key_file=/etc/google_default_creds.json",
            "cp", "combined_daintree_output_config.json", 
            f"gs://{bucket_name}/analysis-pipeline/combined_daintree_output_config-{timestamp}.json"
        ])
        
    """

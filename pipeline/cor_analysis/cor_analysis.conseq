
rule create_cor_analysis_pairs:
    inputs:
        a_set=all {'type': 'cor-analysis-a'},
        b_set=all {'type': 'cor-analysis-b'},
        script=fileref("create_cor_analysis_pairs.py")
    
    run "bash" with """
        cat > inputs.json <<EOF
        {{inputs|quoted}}
        EOF
        """
    run "python3 {{inputs.script.filename}} inputs.json results.json"

rule run_cor_analysis_pair:
    resources: {'slots': "0.05"} # let up to 20 of these run in parallel
    inputs:
        pair={'type': 'cor_input_pair'},
        script=fileref("correlation_with_qvalue.py"),
        sparkles_config=fileref('../sparkles-config')
    outputs:
        {
            "type": "cor_table", 
            "a_given_id": "{{inputs.pair.a_given_id}}",
            "b_given_id": "{{inputs.pair.b_given_id}}", 
            "a_taiga_id": "{{inputs.pair.a_taiga_id}}",
            "b_taiga_id": "{{inputs.pair.b_taiga_id}}",
            "a_feature_id_format": "{{inputs.pair.a_feature_id_format}}",
            "a_features_taiga_id": "{{inputs.pair.a_features_taiga_id|default('')}}",
            "a_compounds_taiga_id": "{{inputs.pair.a_compounds_taiga_id|default('')}}",
            "b_feature_id_format": "{{inputs.pair.b_feature_id_format}}",
            "b_features_taiga_id": "{{inputs.pair.b_features_taiga_id|default('')}}",
            "b_compounds_taiga_id": "{{inputs.pair.b_compounds_taiga_id|default('')}}",
            "filename": {"$filename": "{{inputs.pair.a_given_id}}.{{inputs.pair.b_given_id}}.cor" }
        }

    # write inputs as json file
    run "bash" with """
        cat > inputs.json <<EOF
        {{inputs.pair|quoted}}
        EOF
        """

    # submit sparkles job that does a no-op if the job exists. This is to avoid re-running jobs
    run 'bash' with """
        {{ config.sparkles_path }} \
        --config {{ inputs.sparkles_config.filename }} \
        sub \
        -i us.gcr.io/broad-achilles/depmap-pipeline-run:v4 \
        -u inputs.json \
        -u {{ inputs.script.filename }}:cor.py
        --skipifexists \
        -n {{ inputs.pair.job_name }} \
        python ./cor.py inputs.json {{inputs.pair.a_given_id}}.{{inputs.pair.b_given_id}}.cor
    """

    # if the job isn't complete, wait for it to finish
    run "bash" with """
        {{ config.sparkles_path }} --config {{ inputs.sparkles_config.filename }} watch {{ inputs.pair.job_name }} --loglive
    """

    # copy the results back
    run "bash" with """
        default_url_prefix=$(awk -F "=" '/default_url_prefix/ {print $2}' "{{ inputs.sparkles_config.filename }}")
        gsutil -m cp ${default_url_prefix}/{{job_name}}/1/{{inputs.pair.a_given_id}}.{{inputs.pair.b_given_id}}.cor .
        """


#let publish_dest = "gs://preprocessing-pipeline-outputs/depmap-pipeline/pgm-test"
#let S3_STAGING_URL = "gs://preprocessing-pipeline-outputs/depmap-pipeline/pgm-test/staging"
#rule publish_cor_tables:
#    inputs: cor_tables=all {"type": "cor_table"}
#    publish: "{{config.publish_dest}}/cor_tables.json"

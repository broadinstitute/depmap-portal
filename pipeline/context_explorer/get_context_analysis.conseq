
rule get_context_analysis:
    inputs:
        script=fileref("./get_context_analysis.py"),
        artifacts=all {"type" ~ "subtype_tree|subtype_context_matrix|repurposing_matrix_taiga_id|repurposing_list_taiga_id|prism_oncref_auc_matrix"},
#        subtype_tree_taiga_id=all {"type":"subtype_tree"},
#       context_matrix_taiga_id=all {"type":"subtype_context_matrix"},
        gene_effect_taiga_id={"type":"raw-dep-matrix", "label": 'Chronos_Combined'},
        gene_dependency_taiga_id={"type":"raw-dep-prob-matrix", "label": 'Chronos_Combined'},
#        repurposing_matrix_taiga_id=all {"type": "repurposing_matrix_taiga_id"},
#        repurposing_list_taiga_id=all {"type": "repurposing_list_taiga_id"},
#        oncref_auc_taiga_id=all {"type":"prism_oncref_auc_matrix"},
        compound_summary_repurposing={"type": "compound-summary", "dataset": "Rep_all_single_pt"},
        compound_summary_oncref={"type": "compound-summary", "dataset": "Prism_oncology_AUC"},
        tda_table={"type":"tda-table"},
        workflow=fileref("parallelized_get_context_analysis.json")
    outputs:
        {"type": "context_analysis", "filename": { "$filename": "context_analysis.csv"} }
    run "python3" with """
        import json
    
        artifacts = {{ inputs.artifacts }}

        # transformed will be our newly constructed dict of name -> artifact
        transformed = {
            # handle the ones that couldn't uniquely be identified by type specially
            "repurposing_table_path": [{{ inputs.compound_summary_repurposing }}],
            "oncref_table_path": [{{ inputs.compound_summary_oncref }}],
            "gene_effect_taiga_id": [ {{ inputs.gene_effect_taiga_id }} ],
            "gene_dependency_taiga_id": [ {{ inputs.gene_dependency_taiga_id }} ],
            "tda_table": [ {{ inputs.tda_table }} ],
            "script": {{ inputs.script }}            
        }

        by_type = { artifact['type'] : artifact for artifact in artifacts }

        # now unpack those inputs into ids the script was using
        for dest_name, type_name in [
                ('subtype_tree_taiga_id','subtype_tree'), 
                ('context_matrix_taiga_id', 'subtype_context_matrix'),   
                ('repurposing_matrix_taiga_id', 'repurposing_matrix_taiga_id'), 
                ('repurposing_list_taiga_id', 'repurposing_list_taiga_id'), 
                ('oncref_auc_taiga_id', 'prism_oncref_auc_matrix')]:
            artifact = by_type.get(type_name)
            transformed[dest_name] = [ artifact ] if artifact is not None else []

        raise Exception("todo: transformed needs to map filenames to the parameters below")

        with open("inputs.json", "wt") as fd:
            fd.write(json.dumps(transformed, indent=2))

    """
    run """sparkles --config sparkles-config \
      workflow run test-context-analysis \
     --add-hash-to-job-id \
     --retry \
     --nodes 10 \
     -p batches=10 \
     -u inputs.json \
     -u ~/.taiga/token:.taiga-token \
     -u {{ inputs.script.filename }}:get_context_analysis.py \
     -u {{ }}:rep-cpmd-sum \
     -u {{ }}:oncref-cpmd-sum \
     -u {{ }}:tda-table \
     {{ inputs.workflow }}
"""
    run """gsutil cp `cat results_path.txt` context_analysis.csv """

# Our goal is to run the ensemble pipeline on three datasets:
#  1. the "default crispr" dataset
#  2. the combined RNAi dataset
#  3. The Achilles Chronos dataset (Just a few models, we need to run the "Unbiased" and "Related" ensemble models for gene confidence page)

# --- TESTING ---
#
# The "testpred" variable below can be used to run the ensemble pipeline on a small subset of data. (The first 5 columns of each matrix)
# the way that this works is by preprocessing all the biomarker-matrix-csv -> pred-biomarker-matrix-csv artifacts. If the variable
# is 'true', this will subset the matrix. If not, it will just publish a new artifact which points to the full file. The downstream
# rules take in pred-biomarker-matrix-csv and pred-dep-matrix as inputs

let testpred=""

# Create different model configs for each target matrix (artifacts of type dep-matrix-ftr-unfiltered)
rule process_model_config:
    inputs:
        data={"type": "dep-matrix-ftr-unfiltered"},
        model_config=fileref("model-config.yaml")
    outputs:
        {
        "type": "ensemble-model-config",
        "label": "{{ inputs.data.label }}",
        "filename": {"$filename": "model-config.yaml"}
        },
    run "python" with """
        import yaml

        with open("{{ inputs.model_config.filename }}") as f:
            ensemble_config = yaml.load(f, Loader=yaml.SafeLoader)

        # rewrite confounders to use the final confounder label
        for model_name, model_config in ensemble_config.items():
            for k in ["Features", "Required", "Exempt"]:
                if k not in model_config:
                    continue

                if "{{ inputs.data.confounders_label }}" == "none":
                    model_config[k] = [ r for r in model_config[k] if r != "Confounders" ]
                else:
                    model_config[k] = [
                        r if r != "Confounders" else "{{ inputs.data.confounders_label }}"
                        for r in model_config[k]
                    ]
        
        if "{{ inputs.data.label }}" == "Achilles_for_gene_confidence":
            models_to_use = ["Core_omics", "Related"]
        elif "{{ inputs.data.label }}" in ["Rep1M", "Rep_all_single_pt", "Prism_oncology_AUC"]:
            models_to_use = ["Core_omics", "Extended_omics", "DNA_based"]
        else:
            assert "{{ inputs.data.label }}" in ['Chronos_Combined', 'Chronos_Achilles', 'Avana', 'RNAi_merged', 'CERES_Combined']
            models_to_use = ["Core_omics", "Related", "DNA_based"]
        
        ensemble_config = {
            model_name: model_config
            for model_name, model_config in ensemble_config.items()
            if model_name in models_to_use
        }
        
        with open("model-config.yaml", "w") as f:
            yaml.dump(ensemble_config, f, sort_keys=True)
    """

# Generate dep-matrix-ftr-unfiltered artifacts for only those dep-matrix artifacts which we want to predict
rule download_dep_matrix:
    inputs:
        dep={'type': 'dep-matrix', "label": label},
        probs={"type": "raw-dep-prob-matrix", "label" : label},
        hdf5_utils=fileref("../scripts/hdf5_utils.py", copy_to="hdf5_utils.py")
    run "python3" with """
        from hdf5_utils import read_hdf5
        import re

        df = read_hdf5("{{ inputs.dep.filename }}")
        df = df.T
        df.index.name = "Row.name"

        # RNAi_merged has features which represent genes which have been merged together because they don't have unique gene solutions. These will fail in the ensemble pipeline
        # and so let's filter them out before we pass this data downstream. (These columns can be identified by having a "&" in the gene symbol)
        if "RNAi_merged" == "RNAi_merged":
            GENE_LABEL_FORMAT = re.compile(r"^[a-zA-Z\d\-]+ \(\d+\)$")
            good_columns = [GENE_LABEL_FORMAT.match(column_name) is not None for column_name in df.columns]
            print("Dropping", sum([not x for x in good_columns]), "columns due to merged gene solutions")
            print("before")
            print(df)
            df = df.loc[:,good_columns]
            print("after")
            print(df)

        df.dropna(how="all", axis=0)
        df.dropna(how="all", axis=1)

        df.reset_index().to_feather("dep.ftr")
    """
    run "python" with """
        import subprocess
        import json
        outputs = []

        # only create artifacts for those dep datasets we need to create predictions for
        if "{{inputs.dep.label}}" in ["Chronos_Combined", "RNAi_merged", "Chronos_Achilles"]:
            label = "{{ inputs.dep.label }}"

            if label == "Chronos_Achilles":
                # override the label because the new label is configured to only run on the "Unbiased" and "Related" model
                run_interpretable_models = "false"
                label = "Achilles_for_gene_confidence"
            else:
                run_interpretable_models = "true"

            outputs.append(
                {
                    "type": "dep-matrix-ftr-unfiltered",
                    "dataset_id": "{{ inputs.dep.orig_dataset_id }}",
                    "label": label,
                    "confounders_label": "{{ inputs.dep.confounders_label }}",
                    "has_related": "true",
                    "run_interpretable_models": run_interpretable_models,
                    "filename": {"$filename": "dep.ftr"}
                }
            )

        with open("results.json", "wt") as fd:
            fd.write(json.dumps({"outputs":outputs}))
    """


# Create the "y" matrix containing the genes to predict.
rule filter_dep_ftr:
    executor: dsub {"min_ram":"10", "docker_image": "us-central1-docker.pkg.dev/depmap-consortium/depmap-docker-images/depmap-pipeline-tda:v10"}
    inputs:
        dep={"type": "dep-matrix-ftr-unfiltered"}
    outputs:
        {
            "type": "dep-matrix-ftr",
            "dataset_id": "{{ inputs.dep.dataset_id }}",
            "label": "{{ inputs.dep.label }}",
            "confounders_label": "{{ inputs.dep.confounders_label }}",
            "has_related": "{{ inputs.dep.has_related }}",
            "run_interpretable_models": "{{ inputs.dep.run_interpretable_models }}",
            "filename": {"$filename": "dep-filtered.ftr"}
        }
    run "cds-ensemble prepare-y \
        --input {{ inputs.dep.filename }} \
        --output dep-filtered.ftr"

# Download the table of which genes are "related" to one another. (Used by some model configs)
rule download_match_related:
    inputs:
        data={'type': 'match-related-matrix'}
    outputs:
        {'type': 'match-related-matrix-csv', 'dataset_id': '{{inputs.data.dataset_id}}', 'filename': {"$filename": 'match_related.csv'}}
    run "python3" with """
        from taigapy import create_taiga_client_v3

        tc = create_taiga_client_v3()
        df = tc.get("{{inputs.data.dataset_id}}")
        df.to_csv("match_related.csv")
    """

# reformat biomarker-matrix artifacts into csvs in the format the predictability job is expecting
rule create_biomarker_matrix_csv:
    inputs:
        biomarker_matrix={'type': 'biomarker-matrix'},
        hdf5_utils=fileref('../scripts/hdf5_utils.py', copy_to='hdf5_utils.py'),
        cleanup_script=fileref('../scripts/cleanup_dataframe.py'),
    outputs:
        {
            'type': 'biomarker-matrix-csv',
            'filename': {'$filename': '{{ inputs.biomarker_matrix.category }}.csv'},
            'category': '{{ inputs.biomarker_matrix.category }}',
            "source_dataset_id": "{{ inputs.biomarker_matrix.source_dataset_id }}"
        }
    run "python3" with """
        import pandas as pd
        from hdf5_utils import read_hdf5

        df = read_hdf5("{{ inputs.biomarker_matrix.filename }}")
        df = df.transpose()
        df.to_csv("{{ inputs.biomarker_matrix.category }}.csv")
    """
    run "python3 {{ inputs.cleanup_script.filename }} {{ inputs.biomarker_matrix.category }}.csv --index_col 0"

# the gene effect datasets are only used to test cell line membership
rule compute_crispr_confounders:
    inputs:
        sample_info={"type": "sample_info_dataset_id"},
        params={"type": "crispr-confounder-parameters"},
        confounders_script=fileref("scripts/confounders.py"),
        cleanup_script=fileref("../scripts/cleanup_dataframe.py"),
    outputs:
        {
            "type": "confounders-matrix-csv",
            "label": "crispr",
            "filename": {"$filename": "confounders.csv"},
            "dataset_id": "{{ inputs.sample_info.dataset_id }}",
            "other_dataset_ids": "sample_info: {{ inputs.sample_info.dataset_id }}, achilles_qc_report_taiga_id: {{ inputs.params.achilles_qc_report_taiga_id }}, crispr_screen_map_taiga_id: {{ inputs.params.crispr_screen_map_taiga_id }}"
        }
    run "bash" with """
        python {{ inputs.confounders_script.filename }} \
        {{ inputs.sample_info.dataset_id }} \
        {{ inputs.params.achilles_qc_report_taiga_id }} \
        {{ inputs.params.crispr_screen_map_taiga_id }}
    """
    run "python3 {{ inputs.cleanup_script.filename }} confounders.csv --index_col 0"

rule download_rnai_confounders:
    inputs:
        data={'type': 'confounders-matrix-raw', 'label': 'rnai'},
        cleanup_script=fileref('../scripts/cleanup_dataframe.py'),
    outputs:
        {
            'type': 'confounders-matrix-csv', 
            'label': '{{ inputs.data.label }}', 
            'filename': {'$filename': 'confounders.csv'},
            "dataset_id": "{{ inputs.data.dataset_id }}"
        }
    run "python3" with """
        from taigapy import create_taiga_client_v3
    
        tc = create_taiga_client_v3()
        df = tc.get("{{inputs.data.dataset_id}}")
        df.to_csv("confounders.csv", index=False)
    """
    run "python3 {{ inputs.cleanup_script.filename }} confounders.csv --index_col 0"

rule download_confounders:
    inputs:
        data={'type': 'confounders-matrix-raw', 'label' ~ 'oncref|repallsinglept'},
    outputs:
        {
            'type': 'confounders-matrix-csv', 
            'label': '{{ inputs.data.label }}', 
            'filename': {'$filename': 'confounders.csv'},
            "dataset_id": "{{ inputs.data.dataset_id }}"
        }
    run "python3" with """
        from taigapy import create_taiga_client_v3
    
        tc = create_taiga_client_v3()
        df = tc.get("{{inputs.data.dataset_id}}")
        df.to_csv("confounders.csv")
    """

# convert all confounders-matrix-csv artifacts into biomarker-matrix artifacts to make them consistent with
# all other ensemble input features
rule confounder_matrix_to_biomarker_matrix:
    inputs:
        in={'type': 'confounders-matrix-csv'},
        hdf5_utils=fileref('../scripts/hdf5_utils.py')
    outputs:
        {
            'type': 'biomarker-matrix',
            'category': '{{inputs.in.label}}-confounders',
            'filename': {'$filename': 'out.hdf5'},
            'source_dataset_id': '{{inputs.in.dataset_id}}'
        }
    run "python {{inputs.hdf5_utils.filename}} --transpose to_hdf5 {{inputs.in.filename}} csv out.hdf5"

# conditional rules which will subset the data based on the value of "testprod". See "TESTING" at the top
# of this file for more information.

if "config.get('testpred', 'false') == 'true'":
    let subset_csv = """
        import pandas as pd

        df = pd.read_csv("{{inputs.in.filename}}", index_col=0)
        df.iloc[:,:5].to_csv("out.csv")
    """

    rule make_pred_biomarker_matrix:
        inputs:
            in={'type': 'biomarker-matrix-csv'}
        outputs:
            {'type':'pred-biomarker-matrix-csv',
            'testpred':"{{ config.testpred }}",
            'category':'{{inputs.in.category}}',
            'filename': {"$filename": "out.csv"},
            'source_dataset_id': '{{inputs.in.source_dataset_id}}'}
        run "python" with "{{config.subset_csv}}"

    rule make_pred_dep_matrix:
        inputs:
            in={'type': 'dep-matrix-csv'}
        outputs:
            {'type':'pred-dep-matrix-csv',
            'testpred':"{{ config.testpred }}",
            'label':'{{inputs.in.label}}',
            'confounders_label':'{{inputs.in.confounders_label | default("missing")}}',
            'filename': {"$filename": "out.csv"}, 'dataset_id': '{{inputs.in.dataset_id}}'}
        run "python" with "{{config.subset_csv}}"

    rule make_pred_dep_ftr:
        inputs:
            in={'type':'dep-matrix-ftr'}
        outputs: {"confounders_label":"{{inputs.in.confounders_label}}",
                'type':'pred-dep-matrix-ftr',
                'testpred':"{{ config.testpred }}",
                "dataset_id":"{{inputs.in.dataset_id}}",
                "filename":{"$filename": "out.ftr"},
                "has_related":"{{inputs.in.has_related}}",
                "label":"{{inputs.in.label}}",
                "run_interpretable_models":"{{inputs.in.run_interpretable_models}}"}
        run "python" with """
        import pandas as pd

        df = pd.read_feather("{{inputs.in.filename}}")
        df.iloc[:,:6].to_feather("out.ftr")
        """
else:
    rule make_pred_biomarker_matrix:
        inputs:
            in={'type': 'biomarker-matrix-csv'}
        outputs:
            {'type':'pred-biomarker-matrix-csv',
            'testpred':"{{ config.testpred }}",
            'category':'{{inputs.in.category}}',
            'filename': {"$filename": "{{ inputs.in.filename }}"},
            'source_dataset_id': '{{inputs.in.source_dataset_id}}'}

    rule make_pred_dep_matrix:
        inputs:
            in={'type': 'dep-matrix-csv'}
        outputs:
            {'type':'pred-dep-matrix-csv',
            'testpred':"{{ config.testpred }}",
            'label':'{{inputs.in.label}}',
            'confounders_label':'{{inputs.in.confounders_label | default("missing")}}',
            'filename': {"$filename": "{{ inputs.in.filename }}"},
            'dataset_id': '{{inputs.in.dataset_id}}'}

    rule make_pred_dep_ftr:
        inputs:
            in={'type':'dep-matrix-ftr'}
        outputs: {"confounders_label":"{{inputs.in.confounders_label}}",
            'type':'pred-dep-matrix-ftr',
            'testpred':"{{ config.testpred }}",
            "dataset_id":"{{inputs.in.dataset_id}}",
            'filename': {"$filename": "{{ inputs.in.filename }}"},
            "has_related":"{{inputs.in.has_related}}",
            "label":"{{inputs.in.label}}",
            "run_interpretable_models":"{{inputs.in.run_interpretable_models}}"}

endif

# this is just a stub artifact to allow the following rule to run in cases where the confounder matrix is "none"
add-if-missing {
    "type": "confounders-stub",
    "category": "none"
}

rule prep_rppa_pred_biomarker_matrix:
    inputs:
        rppa={'type': 'harmonized-rppa'},
        hdf5_utils=fileref("../scripts/hdf5_utils.py", copy_to="hdf5_utils.py")
    outputs: {
        'type':	'biomarker-matrix',
        'source_dataset_id': '{{inputs.rppa.dataset_id}}',	
        'category':	'rppa',
        'filename':	{'$filename': 'rppa.hdf5'},
        'testpred': ''
        }
    run "python3" with """
        import hdf5_utils
        from taigapy import create_taiga_client_v3

        tc = create_taiga_client_v3()
        df = tc.get("{{inputs.rppa.dataset_id}}")
        hdf5_utils.write_hdf5(df.transpose(), "rppa.hdf5")
    """


# Construct the feature matrix for each target matrix (and its corresponding model config)
rule assemble_feature_matrix:
    executor: dsub {"min_ram":"15", "docker_image": "us-central1-docker.pkg.dev/depmap-consortium/depmap-docker-images/depmap-pipeline-tda:v10"}
    inputs:
        expression={'type': 'pred-biomarker-matrix-csv', 'category': 'expression'},
        # ssgsea={'type': 'pred-biomarker-matrix-csv', 'category': 'ssgsea'},
        cn={'type': 'pred-biomarker-matrix-csv', 'category': 'copy-number-relative'},
        damaging_mutations={'type': 'pred-biomarker-matrix-csv', 'category': 'mutations-damaging'},
        hotspot_mutations={'type': 'pred-biomarker-matrix-csv', 'category': 'mutations-hotspot'},
        fusions={'type': 'pred-biomarker-matrix-csv', 'category': 'fusions'},
        lineage={'type': 'pred-biomarker-matrix-csv', 'category': 'context'},
        metabolomics={'type':'pred-biomarker-matrix-csv', 'category': 'metabolomics'},
        match_related={'type': 'match-related-matrix-csv'},
        rppa={'type':'pred-biomarker-matrix-csv', 'category': 'rppa'},
        dep_ftr={'type': 'pred-dep-matrix-ftr', "label": label, "confounders_label" : confounders_label},
        model_config={"type": "ensemble-model-config", "label": label},
        confounders={'type' ~ 'pred-biomarker-matrix-csv|confounders-stub', 'category': confounders_label},
#    construct-cache-key-run "python3" with """
#        import json
#        import hashlib
#
#        def get_md5(filename):
#            with open(filename, "rb") as fd:
#                return hashlib.md5(fd.read()).hexdigest()
#
#        key = {"version": 1}
#        inputs = {{ inputs }}
#        for k, v in inputs.items():
#            if "source_dataset_id" in v:
#                key[k+"_source_dataset_id"] = v["source_dataset_id"]
#            elif "dataset_id" in v:
#                key[k+"_source_dataset_id"] = v["dataset_id"]
#            else:
#                assert "filename" in v
#                key[k+"_hash"] = get_md5(v["filename"])
#
#        with open("conseq-cache-key.json", "wt") as fd:
#            fd.write(json.dumps(key))
#    """
    run "python3" with """
        import pandas as pd
        import yaml
        import subprocess
        import json

        with open("{{ inputs.model_config.filename }}") as f:
            ensemble_config = yaml.load(f, Loader=yaml.SafeLoader)

        needs_related = False
        all_features = set(["MatchRelated"])
        for config in ensemble_config.values():
            all_features.update(config['Features'])
            all_features.update(config['Required'])
            if config['Relation'] == "MatchRelated":
                needs_related = True

        filename_mapping = dict(
            RNAseq="{{ inputs.expression.filename }}",
            CN="{{ inputs.cn.filename }}",
            MutDam="{{ inputs.damaging_mutations.filename }}",
            MutHot="{{ inputs.hotspot_mutations.filename }}",
            Fusion="{{ inputs.fusions.filename }}",
            Lin="{{ inputs.lineage.filename }}",
            MatchRelated="{{ inputs.match_related.filename }}",
            RPPA="{{ inputs.rppa.filename }}",
            metabolomics="{{ inputs.metabolomics.filename }}"
        )

        confounders = {{ inputs.confounders }}
        if "{{ inputs.dep_ftr.confounders_label }}" != "none":
            filename_mapping[confounders["category"]] = confounders["filename"]

        all_features = sorted(all_features)

        df = pd.DataFrame(
            {
                "dataset": all_features,
                "filename": [filename_mapping[x] for x in all_features],
            }
        )

        df.to_csv("feature-info.csv", index=False)

        command = ["cds-ensemble", "prepare-x",
            "--output", "X.ftr",
            "--model-config", "{{ inputs.model_config.filename }}",
            "--targets", "{{ inputs.dep_ftr.filename }}",
            "--feature-info", "feature-info.csv"]
        if "{{ inputs.confounders.category }}" != "none":
            command.extend(["--confounders", "{{ inputs.confounders.category }}"])

        if needs_related:
            command.extend(["--output-related", "related"])

        subprocess.check_call(command)

        result = {
            "type": "ensemble-feature-set",
            "filename": { "$filename": "X.ftr" },
            "feature_metadata_filename": { "$filename": "X_feature_metadata.ftr" },
            "valid_samples_filename": { "$filename": "X_valid_samples.ftr" },
            "feature_info_filename": { "$filename": "feature-info.csv" },
            "label": "{{ inputs.dep_ftr.label }}",
            # record the provenance of the input datatypes for debugging
            "expression_dataset_id": "{{ inputs.expression.source_dataset_id }}",
            "cn_dataset_id": "{{ inputs.cn.source_dataset_id }}",
            "damaging_mutations_dataset_id": "{{ inputs.damaging_mutations.source_dataset_id }}",
            "hotspot_mutations_dataset_id": "{{ inputs.hotspot_mutations.source_dataset_id }}",
            "fusions_dataset_id":  "{{ inputs.fusions.source_dataset_id }}",
            "lineage_dataset_id":  "{{ inputs.lineage.source_dataset_id }}",
            "confounders_dataset_ids": "{{ inputs.confounders.dataset_ids | default('missing') }}",
            "match_related_dataset_id": "{{ inputs.match_related.dataset_id }}",
            "dependency_dataset_id": "{{ inputs.dep_ftr.dataset_id }}",
        }

        if needs_related:
            result.update({"related_filename": { "$filename": "related.ftr" } })

        with open("results.json", "wt") as fd:
            fd.write(json.dumps({"outputs": [result]}))

    """

rule fit_predictive_model:
    resources: {'slots': "0.01"}
    inputs:
        match_related={'type': 'match-related-matrix-csv'},
        dep={'type': 'pred-dep-matrix-ftr',
            'label': label,
            "confounders_label": confounders_label, "label": label},
        ensemble_feature_set={"type": "ensemble-feature-set", "label": label},
        model_config={"type": "ensemble-model-config", "label": label},
        compute_hash_script=fileref('../scripts/compute_hash.py'),
        partition_inputs_script=fileref('scripts/partition_ensemble_inputs.py'),
        validate_jobs_script=fileref('scripts/validate_jobs_complete.py'),
        gather_ensemble_script=fileref('scripts/gather_ensemble_tasks.py'),
        sparkles_config=fileref('../sparkles-config')
    outputs: {
        'type': 'pred-models-csv',
        'filename': {'$filename': 'ensemble.csv'},
        'dataset': '{{ inputs.dep.label }}',
    }
    watch-regex: "(?:! .*)|task.*|Submitting job: .*" # print out the task status summary from sparkles or lines that start with a "!"
    run """touch no-related-ftr"""

    run "python3" with """
        print("Running ensemble pipeline for {{ inputs.dep.label }}")
    """

    # when running compute_hash, pipe output through sed which which will prepend the line with a "!" 
    # which matches the watch-regex above and will cause that output to be written to the log
    run """python3 {{ inputs.compute_hash_script.filename }} \
        -u {{ inputs.dep.filename }}:target.ftr \
        -u {{ inputs.model_config.filename }}:model-config.yaml \
        -u {{ inputs.ensemble_feature_set.filename }}:X.ftr \
        -u {{ inputs.ensemble_feature_set.related_filename | default('no-related-ftr') }}:related.ftr \
        -u {{ inputs.ensemble_feature_set.feature_metadata_filename }}:X_feature_metadata.ftr \
        -u {{ inputs.ensemble_feature_set.valid_samples_filename }}:X_valid_samples.ftr \
        --len 10 \
        -o job-hash.txt | sed -e 's/^/! /'
    """
    run "python3" with """
        print("Partitioning inputs for {{ inputs.dep.label }}")
    """
    run "python3 {{ inputs.partition_inputs_script.filename }}\
        {{ inputs.dep.filename }} {{ inputs.model_config.filename }}"

    run "python3" with """
        print("Submitting ensemble pipeline for {{ inputs.dep.label }}")
    """

    run "bash" with """
        HASH=`cat job-hash.txt`

        ADD_RELATED=""
        if [ "{{ inputs.dep.has_related }}" == "true" ]; then
            ADD_RELATED="--related-table related.ftr"
        fi

        JOB_NAME="ensemble_${HASH}"
        # echo the job name so that it gets logged into the conseq output
        echo "! Submitting job as ${JOB_NAME}"

        set -ex
        {{ config.sparkles_path }} \
        --config {{ inputs.sparkles_config.filename }} \
        sub \
        -i us-central1-docker.pkg.dev/depmap-consortium/depmap-docker-images/depmap-pipeline-tda:v6 \
        -u {{ inputs.dep.filename }}:target.ftr \
        -u {{ inputs.model_config.filename }}:model-config.yaml \
        -u {{ inputs.ensemble_feature_set.filename }}:X.ftr \
        -u {{ inputs.ensemble_feature_set.related_filename | default('no-related-ftr') }}:related.ftr \
        -u {{ inputs.ensemble_feature_set.feature_metadata_filename }}:X_feature_metadata.ftr \
        -u {{ inputs.ensemble_feature_set.valid_samples_filename }}:X_valid_samples.ftr \
        -u partitions.csv \
        --params partitions.csv \
        --skipifexists \
        --nodes 300 \
        -n ${JOB_NAME} \
        cds-ensemble fit-model \
        --x X.ftr \
        --y target.ftr \
        --model-config model-config.yaml \
        --n-folds 5 \
        ${ADD_RELATED} \
        --feature-metadata X_feature_metadata.ftr \
        --model-valid-samples X_valid_samples.ftr \
        --target-range {start} {end} \
        --model {model} \
    """

    run "python3" with """
        print("Watching ensemble pipeline for {{ inputs.dep.label }}")
    """

    run "bash" with """
        set -ex
        HASH=`cat job-hash.txt`
        {{ config.sparkles_path }} --config {{ inputs.sparkles_config.filename }} watch ensemble_${HASH} --loglive
        {{ config.sparkles_path }} --config {{ inputs.sparkles_config.filename }} reset ensemble_${HASH} 
        {{ config.sparkles_path }} --config {{ inputs.sparkles_config.filename }} watch ensemble_${HASH} --loglive
        mkdir data
        default_url_prefix=$(awk -F "=" '/default_url_prefix/ {print $2}' "{{ inputs.sparkles_config.filename }}")
        gsutil ls ${default_url_prefix}/ensemble_${HASH}/*/*.csv > completed_jobs.txt
        python {{ inputs.validate_jobs_script.filename }} completed_jobs.txt partitions.csv features.csv predictions.csv
        gsutil -m cp ${default_url_prefix}/ensemble_${HASH}/*/*.csv data
    """

    run "python3" with """
        print("Gathering ensemble pipeline for {{ inputs.dep.label }}")
    """
    
    run "python {{ inputs.gather_ensemble_script.filename }}  \
         {{ inputs.dep.filename }} data partitions.csv features.csv predictions.csv ensemble.csv"

rule convert_predictability_feature_metadata:
    inputs:
        feature_metadata={"type": "ensemble-feature-set"}
    outputs:
        {
            "type": "ensemble-feature-metadata",
            "label": "{{ inputs.feature_metadata.label }}",
            "filename": { "$filename": "feature_metadata.csv"}
        }
    run "python" with """
        import pandas as pd

        df = pd.read_feather("{{ inputs.feature_metadata.feature_metadata_filename }}")
        df.to_csv("feature_metadata.csv", index=False)
    """

##########################
# Rules related to processing compounds
##################

rule compound_matrix_to_ftr:
    inputs:
        data={
            "type": "dep-matrix",
            "label" ~ "Rep_all_single_pt|Prism_oncology_AUC"
        },
        hdf5_utils=fileref("../scripts/hdf5_utils.py", copy_to="hdf5_utils.py")
    outputs:
        {
            "type": "dep-matrix-ftr-unfiltered",
            "dataset_id": "{{ inputs.data.orig_dataset_id }}",
            "label": "{{ inputs.data.label }}",
            "confounders_label": "{{ inputs.data.confounders_label }}",
            "has_related": "false",
            "run_interpretable_models": "false",
            "filename": {"$filename": "prism_sensitivity.ftr"}
        }

    run "python3" with """
        from hdf5_utils import read_hdf5

        df = read_hdf5("{{ inputs.data.filename }}")
        df = df.T
        df.index.name = "Row.name"

        df.dropna(how="all", axis=0)
        df.dropna(how="all", axis=1)

        df.reset_index().to_feather("prism_sensitivity.ftr")
    """


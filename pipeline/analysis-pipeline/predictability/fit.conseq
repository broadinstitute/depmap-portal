# Three Steps:
# 1. Generate a daintree input config file for each model and screen
# 2. Run the model fitting
# 3. Combine the output config files

let sparkes_path = "/install/sparkles/bin/sparkles"
let daintree_docker_image = "us-central1-docker.pkg.dev/cds-docker-containers/docker/daintree:test"
let dest_permaname = "internal-25q2-pred-test-f0ee"

rule process_model_config:
    inputs:
        # Model config yaml file
        model_config=fileref("model-config.yaml"),

        # Target matrices
        crispr_gene_effect={"type": "target_matrix", "label": "crispr_gene_effect"},
        rnai={"type": "target_matrix", "label": "rnai"},
        oncref={"type": "target_matrix", "label": "oncref"},

        # Features
        lineage={"type": "feature", "label": "lineage"},
        crispr_confounder={"type": "feature", "label": "crispr_confounder"},
        rnai_confounder={"type": "feature", "label": "rnai_confounder"},
        oncref_confounder={"type": "feature", "label": "oncref_confounder"},
        driver_events={"type": "feature", "label": "driver_events"},
        armlevel_cna={"type": "feature", "label": "armlevel_cna"},
        cytoband_cn={"type": "feature", "label": "cytoband_cn"},
        genetic_signature={"type": "feature", "label": "genetic_signature"},
        mutations_hotspot={"type": "feature", "label": "mutations_hotspot"},
        mutations_damaging={"type": "feature", "label": "mutations_damaging"},
        gene_cn={"type": "feature", "label": "gene_cn"},
        loh={"type": "feature", "label": "loh"},
        rnaseq={"type": "feature", "label": "rnaseq"},

        # Script to generate daintree input config file
        script=fileref("scripts/generate_daintree_input_configs.py"),

    run "python" with """
    import json

    config_dict = {{inputs}}
    with open("daintree_input_config.json", 'w') as f:
        json.dump(config_dict, f, indent=2)   

    """
    # Add --test-only-first-n 2 to only generate a two configs (for testing)
    run "python3 {{ inputs.script.filename }} --model_config {{ inputs.model_config.filename }} --input_config 'daintree_input_config.json'"


rule run_fit_models:
    resources: {'slots': "0.1"} # let up to 10 of these run in parallel
    inputs:
        daintree_input_config={
          "type": "daintree_input_config"
          },
        release_taiga_id={
          "type": "release_taiga_id"
          },
        sparkles_config=fileref("sparkles-config"),
        create_workflow_script=fileref("scripts/create_daintree_workflow.py")
    run "python3 {{ inputs.create_workflow_script.filename }} --config {{ inputs.daintree_input_config.filename }} --out workflow.json --nfolds 5 --models-per-task {{ inputs.daintree_input_config.models_per_task}} --test-first-n-tasks 10"
    run "cp {{ inputs.sparkles_config.filename }} .sparkles"
    run "{{config.sparkles_path}} workflow run --nodes 100 --retry --add-hash-to-job-id -i {{ config.daintree_docker_image }} daintree-fit-v1 workflow.json"

    # reformat daintree-output into artifacts conseq will understand
    run "python3" with """
    import json

    with open("daintree-output.json", "rt") as fd:
        result = json.load(fd)

    with open("results.json", "wt") as fd:
        fd.write(json.dumps(
            {"outputs": [
                {
                    "type": "daintree_output",
                    "model_name": {{ inputs.daintree_input_config.model_name | quoted }},
                    "screen_name": {{ inputs.daintree_input_config.screen_name | quoted }},
                    "sparkles_job_name": {"$value": result["sparkles_job_name"]},
                    "daintree_input_config_filename": {{ inputs.daintree_input_config.filename | quoted }},
                    "features_metadata_filename" : {"$file_url": result["features_metadata_path"]},
                    "ensemble_filename" : {"$file_url": result["ensemble_path"]},
                    "predictions_filename": {"$file_url": result["predictions_path"]}}
            ]}))
    """

rule upload_fit_results_to_taiga2:
    inputs:
        daintree_output={"type": "daintree_output"},
        script=fileref("scripts/upload_fit_results_to_taiga.py")
    outputs:
        {
            "type": "daintree_upload_output", 
            "filename": {"$filename": "{{ inputs.daintree_output.model_name }}-{{ inputs.daintree_output.screen_name }}.json" },
            "model_name": "{{ inputs.daintree_output.model_name }}",
            "screen_name": "{{ inputs.daintree_output.screen_name }}"
        }
    run "bash" with """
        python3 {{inputs.script.filename}} \
            --runner_config_file {{ inputs.daintree_output.daintree_input_config_filename }} \
            --features_metadata_csv {{ inputs.daintree_output.features_metadata_filename }} \
            --ensemble_csv {{ inputs.daintree_output.ensemble_filename }} \
            --predictions_matrix_csv {{ inputs.daintree_output.predictions_filename }} \
            --dest_permaname {{ config.dest_permaname }} \
            --output_config_file {{ inputs.daintree_output.model_name }}-{{ inputs.daintree_output.screen_name }}.json \
    """

rule combine_daintree_upload_outputs:
    inputs:
        outputs = all{
          "type": "daintree_upload_output"
          }
    outputs:
        {
          "type": "combined_daintree_upload_outputs",
          "filename": {"$filename": "combined_daintree_upload_outputs.json"}
        }
    run "python" with """
        import json
        import os

        def merge_json_files(json_files):
            combined = {}
            
            for file_path in json_files:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    
                model_names = list(data.keys())
                assert len(model_names) == 1
                model_name = model_names[0]
                screen_name = data[model_name]["input"]["screen_name"]
                
                # Initialize the screen in combined if it doesn't exist
                if screen_name not in combined:
                    combined[screen_name] = {}
                    
                # Add the model data to the appropriate screen
                combined[screen_name][model_name] = data[model_name]
            
            return combined        

        artifacts = {{ inputs.outputs | quoted }}
        list_of_files = [artifact['filename'] for artifact in artifacts]

        combined_output_config = merge_json_files(list_of_files)

        with open("combined_daintree_upload_outputs.json", 'w') as f:
            json.dump(combined_output_config, f, indent=2)
    """

# TODO: Add publish rule
